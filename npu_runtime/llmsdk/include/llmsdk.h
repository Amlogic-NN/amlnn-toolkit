/*
* Copyright (C) 2024 Amlogic, Inc. All rights reserved.
*
* Description:
*   This file contains the public API for the LLM SDK.
*/

#ifndef _LLMSDK_H_
#define _LLMSDK_H_
#include <cstdint>

#ifdef __cplusplus
extern "C" {
#endif

#define LLMSDK_VERSION "LLMSDK, v1.0.0, 2025.07" // v<Major>.<Minor>.<Patch>

/**
 * @typedef LLMContext
 * @brief A handle used for managing and interacting with the large language model runtime.
 */
typedef void* LLMContext;

/**
 * @enum AML_LLMRunStatus
 * @brief Represents the possible states during an LLM generation process.
 */
typedef enum {
    AML_LLM_RUN_NORMAL = 0, /**< The LLM is currently processing as expected. */
    AML_LLM_RUN_FINISH = 1, /**< The LLM has completed the task successfully. */
    AML_LLM_RUN_ERROR  = 2, /**< The LLM encountered an error during execution. */
} AML_LLMRunStatus;

/**
 * @enum AML_LLMRetStatus
 * @brief Indicates the return status of an LLM SDK API call.
 */
typedef enum {
    AML_LLM_Status_Success = 0, /**< The API call completed successfully. */
    AML_LLM_Status_Failed  = 1, /**< The API call failed. This is a generic error return value. */
} AML_LLMRetStatus;

/**
 * @struct AML_LLMInitExtend
 * @brief Additional optional parameters for LLM init (currently reserved for future use).
 */
typedef struct {
    uint8_t reserved[128];             /**< Reserved for future extension of init configuration. */
} AML_LLMInitExtend;

/**
 * @enum AML_LLMSamplingMode
 * @brief Defines available sampling strategies for output generation.
 */
typedef enum {
    AML_LLM_ARG_Max, /**< Argmax sampling. Always selects the token with the highest probability. Deterministic and reproducible. */
    AML_LLM_TOP_P,   /**< Top-P (nucleus) sampling. Samples from the smallest set of tokens whose cumulative probability exceeds threshold p. */
    AML_LLM_TOP_K    /**< Top-K sampling. Samples from the top k tokens with highest probabilities. Balances diversity and control. */
} AML_LLMSamplingMode;

/**
 * @struct AML_LLMInitConfig
 * @brief Contains configuration parameters for initializing the LLM instance.
 */
typedef struct {
    const char* model_path;            /**< Path to the LLM model file. */
    const char* tokenizer_path;        /**< Path to tokenizer resources. */
    AML_LLMSamplingMode sampling_mode; /**< Sampling strategy for text generation. */
    int32_t top_k;                     /**< Top-K sampling parameter. */
    float top_p;                       /**< Top-P sampling parameter. */
    float temperature;                 /**< Controls randomness in token selection. */
    float repeat_penalty;              /**< Penalty for repeated tokens. */
    AML_LLMInitExtend init_extend;     /**< Reserved field for future init extension. */
} AML_LLMInitConfig;

/**
 * @enum AML_LLMInputType
 * @brief Specifies the format of input data accepted by the LLM.
 */
typedef enum {
    AML_LLM_INPUT_PROMPT = 0, /**< Input is a plain text prompt string. */
} AML_LLMInputType;

/**
 * @struct AML_LLMInput
 * @brief Represents input data passed to the LLM, supporting multiple input types.
 */
typedef struct {
    AML_LLMInputType input_type;  /**< Specifies the type of input. */
    union {
        const char* prompt_input; /**< Used when input_type is AML_LLM_INPUT_PROMPT. Contains plain text. */
    };
} AML_LLMInput;

/**
 * @enum AML_LLMRunMode
 * @brief Describes the available modes for performing LLM inference.
 */
typedef enum {
    AML_LLM_RUN_GENERATE = 0, /**< Generates text output based on the input prompt. */
} AML_LLMRunMode;

/**
 * @struct AML_LLMRunExtend
 * @brief Additional optional parameters for LLM inference (currently reserved for future use).
 */
typedef struct {
    uint8_t reserved[64]; /**< Reserved for future extension of run-time configuration. */
} AML_LLMRunExtend;

/**
 * @struct AML_LLMRunConfig
 * @brief Configuration structure to control the behavior of a single inference run.
 */
typedef struct {
    AML_LLMRunMode run_mode;     /**< Inference mode (e.g., text generation). */
    int retain_history;          /**< Whether to retain conversation history: 1 to retain, 0 to reset after each run. */
    AML_LLMRunExtend run_extend; /**< Reserved field for future runtime extension. */
} AML_LLMRunConfig;

/**
 * @struct AML_LLMResult
 * @brief Represents the results generated by LLM during inference.
 *
 * This result is updated and delivered via the callback for each token generated.
 * Users should handle the output incrementally (e.g., printing or accumulating externally).
 */
typedef struct {
    const char* text; /**< The text content generated by the current inference. */
    int32_t token_id; /**< The token_id generated by the current inference. */
} AML_LLMResult;

/**
 * @typedef LLMResultCallback
 * @brief Function signature for handling inference results.
 *
 * @param result Pointer to the result data generated by the LLM.
 * @param userdata Pointer to user-defined data passed to the callback.
 * @param state Current status of the inference process.
 */
typedef void(*LLMResultCallback)(AML_LLMResult* result, void* userdata, AML_LLMRunStatus state);


/**
 * @brief Initialize an LLM instance with the given configuration and callback handler.
 *
 * @param context Pointer to the LLM runtime context.
 * @param init_config Pointer to the configuration settings for initialization.
 * @param callback Function to handle results generated by the LLM.
 * @return Status code (0 for success, non-zero indicates failure).
 */
AML_LLMRetStatus aml_llm_init(LLMContext* context, AML_LLMInitConfig* init_config, LLMResultCallback callback);

/**
 * @brief Uninitialize the LLM instance and release associated resources.
 *
 * @param context The LLM context.
 * @return Status code (0 for success, non-zero indicates failure).
 */
AML_LLMRetStatus aml_llm_uninit(LLMContext context);

/**
 * @brief Execute an inference task.
 *
 * For streaming output, results are delivered incrementally via the registered callback.
 *
 * @param context The LLM context.
 * @param input Pointer to input data for the inference.
 * @param run_config Pointer to inference execution parameters.
 * @param userdata User-defined data that will be passed to the callback.
 * @return Status code (0 for success, non-zero indicates failure).
 */
AML_LLMRetStatus aml_llm_run(LLMContext context, AML_LLMInput* input, AML_LLMRunConfig* run_config, void* userdata);

/**
 * @brief Reset the state of the LLM context, clearing any existing history or state.
 *
 * @param context The LLM context.
 * @return Status code (0 for success, non-zero indicates failure).
 */
AML_LLMRetStatus aml_llm_reset(LLMContext context);

/**
 * @brief Interrupt an active inference process.
 *
 * This function attempts to stop any ongoing generation immediately.
 *
 * @param context The LLM context.
 * @return Status code (0 for success, non-zero indicates failure).
 */
AML_LLMRetStatus aml_llm_break(LLMContext context);

/**
 * @brief Configure the chat formatting template for the LLM, including system prompt, prefix, and postfix.
 *
 * This function lets users define how prompts are constructed during chat interactions.
 * The system prompt typically sets the initial context or role of the assistant.
 * The prefix is prepended to each user input, and the postfix is appended after the input,
 * helping structure the prompt for consistent model behavior.
 *
 * @param context The LLM context.
 * @param system_prompt A string that defines the initial context or role of the LLM.
 * @param prompt_prefix A string inserted before each user input.
 * @param prompt_postfix A string appended after each user input.
 *
 * @return Status code (0 on success; non-zero indicates an error).
 */
AML_LLMRetStatus aml_llm_set_chat_template(LLMContext context, const char* system_prompt, const char* prompt_prefix, const char* prompt_postfix);

// ====================================================
// Function Calling Extension
// ====================================================

/**
 * @brief Tool execution callback signature.
 *
 * @param tool_name Name of the tool being invoked
 * @param arguments JSON string of the tool arguments
 * @param userdata Custom user data pointer
 * @param out_result_buffer Pointer to a string buffer to store the JSON result (must be allocated in callback, will be freed by SDK)
 * @return 0 on success, non-zero on failure
 */
typedef int(*LLMToolExecCallback)(const char* tool_name, const char* arguments, void* userdata, char** out_result_buffer);

/**
 * @brief Enable Function Calling mode by setting tool definitions and system prompt.
 *
 * @param context LLM context
 * @param system_prompt System prompt string
 * @param tools_json JSON string describing the tools
 * @param tool_response_tag Tag to identify tool response messages
 * @param fc_external_mode External mode for function calling
 * @return Status code
 */
AML_LLMRetStatus aml_llm_enable_function_calling(LLMContext context, const char* system_prompt, 
    const char* tools_json, const char* tool_response_tag);

/**
 * @brief Register a tool execution callback function.
 *
 * @param context LLM context
 * @param callback Callback function for executing tools
 * @param userdata Custom user data pointer
 * @return Status code
 */
AML_LLMRetStatus aml_llm_register_tool_callback(LLMContext context, LLMToolExecCallback callback, void* userdata);


#ifdef __cplusplus
} // extern "C"
#endif

#endif // _LLMSDK_H_
